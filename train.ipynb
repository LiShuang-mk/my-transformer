{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import MyTrans as trans\n",
    "from utils import *\n",
    "from dataloader import DataSet\n",
    "from tqdm.notebook import tqdm as tqnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransTrans(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        tgt_vocab_size,\n",
    "        max_src_len,\n",
    "        max_tgt_len,\n",
    "        num_heads,\n",
    "        num_encoders,\n",
    "        num_decoders,\n",
    "        dim_model,\n",
    "        dim_feedforward,\n",
    "        device,\n",
    "        padding_idx=0,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super(TransTrans, self).__init__()\n",
    "\n",
    "        self.src_emb = trans.embedding.TransformerEmbedding(\n",
    "            vocab_size=src_vocab_size,\n",
    "            max_len=max_src_len,\n",
    "            d_model=dim_model,\n",
    "            device=device,\n",
    "            padding_idx=padding_idx,\n",
    "        )\n",
    "        self.tgt_emb = trans.embedding.TransformerEmbedding(\n",
    "            vocab_size=tgt_vocab_size,\n",
    "            max_len=max_tgt_len,\n",
    "            d_model=dim_model,\n",
    "            device=device,\n",
    "            padding_idx=padding_idx,\n",
    "        )\n",
    "\n",
    "        self.transformer = torch.nn.Transformer(\n",
    "            d_model=dim_model,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_encoders,\n",
    "            num_decoder_layers=num_decoders,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "            dropout=dropout,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        self.final_linear = torch.nn.Linear(dim_model, tgt_vocab_size)\n",
    "\n",
    "        self.pad_idx = padding_idx\n",
    "\n",
    "        self.tgt_mask = torch.tril(torch.ones(max_tgt_len, max_tgt_len, device=device))\n",
    "        self.tgt_mask[self.tgt_mask == 0] = float(\"-inf\")\n",
    "        self.tgt_mask[self.tgt_mask == 1] = 0\n",
    "\n",
    "    def forward(self, src, tgt, src_len, tgt_len):\n",
    "        src_key_padding_mask = self.get_key_padding_mask(src)\n",
    "        tgt_key_padding_mask = self.get_key_padding_mask(tgt)\n",
    "\n",
    "        src = self.src_emb(src, src_len)\n",
    "        tgt = self.tgt_emb(tgt, tgt_len)\n",
    "\n",
    "        # 将准备好的数据送给transformer\n",
    "        out = self.transformer(\n",
    "            src,\n",
    "            tgt,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            tgt_mask=self.tgt_mask,\n",
    "        )\n",
    "\n",
    "        out = self.final_linear(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def get_key_padding_mask(self, tokens):\n",
    "        key_padding_mask = torch.zeros(tokens.size(), device=tokens.device)\n",
    "        key_padding_mask[tokens == self.pad_idx] = -torch.inf\n",
    "        return key_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use cuda\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"model_dim\": 16,\n",
    "    \"num_layers\": 1,\n",
    "    \"num_heads\": 4,\n",
    "    \"ff_dim\": 256,\n",
    "    \"dropout\": 0.1,\n",
    "    \"batch_size\": 16,\n",
    "    \"learning_rate\": 5e-3,\n",
    "    \"num_epochs\": 100,\n",
    "    \"seed\": 2003,\n",
    "    \"model_path\": \"./models/\",\n",
    "    \"early_stop\": 10,\n",
    "}\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"use {device}\")\n",
    "\n",
    "seed = params[\"seed\"]\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TT = torch.Tensor\n",
    "\n",
    "\n",
    "def normalize_sizes(y_pred: TT, y_true: TT):\n",
    "    if len(y_pred.size()) == 3:\n",
    "        y_pred = y_pred.contiguous().view(-1, y_pred.size(2))\n",
    "    if len(y_true.size()) == 2:\n",
    "        y_true = y_true.contiguous().view(-1)\n",
    "    return y_pred, y_true\n",
    "\n",
    "\n",
    "def compute_loss(y_pred: TT, y_true: TT, mask_idx):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "    return torch.nn.functional.cross_entropy(y_pred, y_true, ignore_index=mask_idx)\n",
    "\n",
    "\n",
    "def compute_accuracy(y_pred: TT, y_true: TT, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "\n",
    "    correct_indices = torch.eq(y_pred_indices, y_true).float()\n",
    "    valid_indices = torch.ne(y_true, mask_index).float()\n",
    "\n",
    "    n_correct = (correct_indices * valid_indices).sum().item()\n",
    "    n_valid = valid_indices.sum().item()\n",
    "\n",
    "    return n_correct / n_valid * 100\n",
    "\n",
    "\n",
    "def start_train(model, ds, dl, dl_val):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params[\"learning_rate\"])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer=optimizer, mode=\"min\", factor=0.5, patience=1\n",
    "    )\n",
    "\n",
    "    epoch_bar = tqnb(desc=\"training routine\", total=params[\"num_epochs\"], position=0)\n",
    "\n",
    "    ds.set_split(\"train\")\n",
    "    train_bar = tqnb(\n",
    "        desc=\"split=train\",\n",
    "        total=ds.get_num_batches(params[\"batch_size\"]),\n",
    "        position=1,\n",
    "        leave=True,\n",
    "    )\n",
    "    ds.set_split(\"val\")\n",
    "    val_bar = tqnb(\n",
    "        desc=\"split=val\",\n",
    "        total=ds.get_num_batches(params[\"batch_size\"]),\n",
    "        position=1,\n",
    "        leave=True,\n",
    "    )\n",
    "\n",
    "    vectorizor = ds.get_vectorizer()\n",
    "\n",
    "    logger = get_root_logger()\n",
    "    loss_val_best = 1e10\n",
    "    now_str = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    "    best_model_state = None\n",
    "    save_cnt = 0\n",
    "\n",
    "    logger.info(params)\n",
    "\n",
    "    for epoch in range(params[\"num_epochs\"]):\n",
    "\n",
    "        ds.set_split(\"train\")\n",
    "        model.train()\n",
    "\n",
    "        loss_epoch, acc_epoch, batch = 0, 0, 0\n",
    "        for data_dict in dl:\n",
    "            x_src = data_dict[\"x_source\"].to(device)\n",
    "            x_tgt = data_dict[\"x_target\"].to(device)\n",
    "            y_tgt = data_dict[\"y_target\"].to(device)\n",
    "            len_x_src = data_dict[\"x_srclen\"].to(device)\n",
    "            len_x_tgt = data_dict[\"x_tgtlen\"].to(device)\n",
    "\n",
    "            #\n",
    "            # x_tgt = torch.zeros_like(x_src).to(device)\n",
    "            # x_tgt[:, :] = vectorizor.tgt_vocab.msk_idx\n",
    "            # x_tgt[:, 0] = vectorizor.tgt_vocab.sos_idx\n",
    "            # x_src_len = torch.tensor(data_dict[\"x_srclen\"], dtype=torch.int64).to(device)\n",
    "            # x_tgt_len = torch.zeros_like(data_dict[\"x_tgtlen\"], dtype=torch.int64).to(device)\n",
    "            # x_tgt_len[:] = 1\n",
    "            #\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(x_src, x_tgt, len_x_src, len_x_tgt)\n",
    "            #\n",
    "            # for i in range(vectorizor.max_tgt_len):\n",
    "            #     y_pred = model(x_src, x_tgt, x_src_len, x_tgt_len)\n",
    "            #     y_pred_indices = torch.argmax(y_pred, dim=-1)\n",
    "            #     x_tgt_len[:] += 1\n",
    "            #     x_tgt = y_pred_indices\n",
    "            #\n",
    "\n",
    "            loss = compute_loss(y_pred, y_tgt, mask_idx=ds.get_src_vocab().msk_idx)\n",
    "            acc = compute_accuracy(y_pred, y_tgt, mask_index=ds.get_src_vocab().msk_idx)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_epoch += loss.detach().cpu().item()\n",
    "            acc_epoch += acc\n",
    "\n",
    "            batch += 1\n",
    "            train_bar.set_postfix(\n",
    "                loss=loss_epoch / batch, acc=acc_epoch / batch, epoch=epoch\n",
    "            )\n",
    "            train_bar.update()\n",
    "\n",
    "        ds.set_split(\"val\")\n",
    "        model.eval()\n",
    "        loss_epoch_val, acc_epoch_val, batch_val = 0, 0, 0\n",
    "\n",
    "        for data_dict in dl_val:\n",
    "            x_src = data_dict[\"x_source\"].to(device)\n",
    "            x_tgt = data_dict[\"x_target\"].to(device)\n",
    "            y_tgt = data_dict[\"y_target\"].to(device)\n",
    "            len_x_src = data_dict[\"x_srclen\"].to(device)\n",
    "            len_x_tgt = data_dict[\"x_tgtlen\"].to(device)\n",
    "\n",
    "            #\n",
    "            # x_tgt = torch.zeros_like(x_src).to(device)\n",
    "            # x_tgt[:, :] = vectorizor.tgt_vocab.msk_idx\n",
    "            # x_tgt[:, 0] = vectorizor.tgt_vocab.sos_idx\n",
    "            # x_src_len = torch.tensor(data_dict[\"x_srclen\"], dtype=torch.int64).to(device)\n",
    "            # x_tgt_len = torch.zeros_like(data_dict[\"x_tgtlen\"], dtype=torch.int64).to(device)\n",
    "            # x_tgt_len[:] = 1\n",
    "            #\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y_pred = model(x_src, x_tgt, len_x_src, len_x_tgt)\n",
    "                #\n",
    "                # for i in range(vectorizor.max_tgt_len):\n",
    "                #     y_pred = model(x_src, x_tgt, x_src_len, x_tgt_len)\n",
    "                #     y_pred_indices = torch.argmax(y_pred, dim=-1)\n",
    "                #     x_tgt_len[:] += 1\n",
    "                #     x_tgt = y_pred_indices\n",
    "                #\n",
    "\n",
    "            loss = compute_loss(y_pred, y_tgt, mask_idx=ds.get_src_vocab().msk_idx)\n",
    "            acc = compute_accuracy(y_pred, y_tgt, mask_index=ds.get_src_vocab().msk_idx)\n",
    "\n",
    "            loss_epoch_val += loss.detach().cpu().item()\n",
    "            acc_epoch_val += acc\n",
    "\n",
    "            batch_val += 1\n",
    "            val_bar.set_postfix(\n",
    "                loss=loss_epoch_val / batch_val,\n",
    "                acc=acc_epoch_val / batch_val,\n",
    "                epoch=epoch,\n",
    "            )\n",
    "            val_bar.update()\n",
    "\n",
    "        logger.info(\n",
    "            f\"Epoch {epoch}: train loss: {loss_epoch/batch}, \"\n",
    "            + f\"train acc: {acc_epoch/batch}, val loss: {loss_epoch_val/batch_val}, \"\n",
    "            + f\"val acc: {acc_epoch_val/batch_val}\"\n",
    "        )\n",
    "\n",
    "        scheduler.step(loss_epoch_val / batch_val)\n",
    "\n",
    "        if loss_epoch_val / batch_val < loss_val_best:\n",
    "            loss_val_best = loss_epoch_val / batch_val\n",
    "            epoch_bar.set_postfix(loss_val_best=loss_val_best)\n",
    "            es = 0\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            if epoch > 10:\n",
    "                torch.save(\n",
    "                    best_model_state,\n",
    "                    params[\"model_path\"]\n",
    "                    + f\"model_{now_str}_{params['seed']}_e{epoch}_d{int(params[\"dropout\"]*100)}.pth\",\n",
    "                )\n",
    "                save_cnt += 1\n",
    "        else:\n",
    "            es += 1\n",
    "\n",
    "        if es >= params[\"early_stop\"]:\n",
    "            print(\"Early stopping!\")\n",
    "            logger.warning(\"early stopping!\")\n",
    "            epoch_bar.close()\n",
    "            train_bar.close()\n",
    "            val_bar.close()\n",
    "            if save_cnt == 0:\n",
    "                torch.save(\n",
    "                    best_model_state,\n",
    "                    params[\"model_path\"]\n",
    "                    + f\"model_{now_str}_{params['seed']}_e{epoch}_d{int(params[\"dropout\"]*100)}.pth\",\n",
    "                )\n",
    "            break\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "\n",
    "    epoch_bar.close()\n",
    "    train_bar.close()\n",
    "    val_bar.close()\n",
    "    del logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DataSet.from_csv(\"./data/eng_fra.csv\")\n",
    "ds.set_split(\"train\")\n",
    "dl = torch.utils.data.DataLoader(\n",
    "    ds, batch_size=params[\"batch_size\"], shuffle=True, num_workers=4\n",
    ")\n",
    "ds.set_split(\"val\")\n",
    "dl_val = torch.utils.data.DataLoader(\n",
    "    ds, batch_size=params[\"batch_size\"], shuffle=False, num_workers=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lishuang/anaconda3/envs/psc-cpi/lib/python3.12/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b29870a0a5c44beaabaf822b5416f183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f771a9abb07045fb840291042a073163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ef3c025e7f74a628cd319dd2cd38dcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"model_dim\": 64,\n",
    "    \"num_layers\": 1,\n",
    "    \"num_heads\": 4,\n",
    "    \"ff_dim\": 256,\n",
    "    \"dropout\": 0.1,\n",
    "    \"batch_size\": 16,\n",
    "    \"learning_rate\": 5e-3,\n",
    "    \"num_epochs\": 100,\n",
    "    \"seed\": 2003,\n",
    "    \"model_path\": \"./models/\",\n",
    "    \"early_stop\": 10,\n",
    "}\n",
    "\n",
    "model = TransTrans(\n",
    "    src_vocab_size=ds.get_src_vocab_size(),\n",
    "    tgt_vocab_size=ds.get_tgt_vocab_size(),\n",
    "    max_src_len=ds.get_max_src_len(),\n",
    "    max_tgt_len=ds.get_max_tgt_len(),\n",
    "    num_heads=params[\"num_heads\"],\n",
    "    num_encoders=params[\"num_layers\"],\n",
    "    num_decoders=params[\"num_layers\"],\n",
    "    dim_feedforward=params[\"ff_dim\"],\n",
    "    dim_model=params[\"model_dim\"],\n",
    "    device=device,\n",
    "    padding_idx=ds.get_src_vocab().msk_idx,\n",
    "    dropout=params[\"dropout\"],\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "start_train(model, ds, dl, dl_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lishuang/anaconda3/envs/psc-cpi/lib/python3.12/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c390457edea6421da64719172c8f9bc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95ddaff5f445445e94ff9238a77b990a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb7abbdf25f848489695c1472c733929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"model_dim\": 32,\n",
    "    \"num_layers\": 1,\n",
    "    \"num_heads\": 4,\n",
    "    \"ff_dim\": 256,\n",
    "    \"dropout\": 0.1,\n",
    "    \"batch_size\": 16,\n",
    "    \"learning_rate\": 5e-3,\n",
    "    \"num_epochs\": 100,\n",
    "    \"seed\": 2003,\n",
    "    \"model_path\": \"./models/\",\n",
    "    \"early_stop\": 10,\n",
    "}\n",
    "\n",
    "model = TransTrans(\n",
    "    src_vocab_size=ds.get_src_vocab_size(),\n",
    "    tgt_vocab_size=ds.get_tgt_vocab_size(),\n",
    "    max_src_len=ds.get_max_src_len(),\n",
    "    max_tgt_len=ds.get_max_tgt_len(),\n",
    "    num_heads=params[\"num_heads\"],\n",
    "    num_encoders=params[\"num_layers\"],\n",
    "    num_decoders=params[\"num_layers\"],\n",
    "    dim_feedforward=params[\"ff_dim\"],\n",
    "    dim_model=params[\"model_dim\"],\n",
    "    device=device,\n",
    "    padding_idx=ds.get_src_vocab().msk_idx,\n",
    "    dropout=params[\"dropout\"],\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "start_train(model, ds, dl, dl_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psc-cpi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
